<!DOCTYPE html>
<html><title>Complex linear regression (in the field ℂ of complex numbers) ⁑ Dercuano</title><meta charset="utf-8"></meta><link href="../liabilities/style.css" rel="stylesheet"></link><meta content="width=device-width, initial-scale=1.0" name="viewport"></meta><h1>Complex linear regression (in the field ℂ of complex numbers)</h1><div class="metadata">Kragen Javier Sitaker, 2019-08-17 (updated 2019-08-18)
(9 minutes)</div>
<p>In <a href="../notes/%25241-recognizer-diagrams.html">$1 recognizer diagrams</a> I speculated that applying linear
regression to vectors of complex numbers might be a good way to match
user interface gestures such as strokes to templates, since a linear
function in ℂ amounts to a translation, rotation, and scaling.  But I
can’t find any discussion of doing linear regression on complex
numbers.</p>
<p>So I’m going to solve that problem here.</p>
<h2>The stroke-matching problem</h2>
<p>Suppose we have a sequence of points representing a stroke the user
has drawn with their pen or a finger on a pen computer — whether
something like the Fly Pen, something like the HP Itsy, something like
a Wacom graphics tablet, or something like a modern Android computer.
We want to determine how far this sequence is from being “the same as”
a stored template representing, say, the letter “B”, so we can see
which of several stored templates it’s closest to and thus determine
the user’s intent.</p>
<p>Let’s suppose the sequence of points has already been resampled to
regular spatial intervals, with the same number of points as the
template (128 or something), so pairing up corresponding points is
trivial.</p>
<p>The simplest thing that could possibly work is to take the sum of
absolute differences of coordinates between corresponding points: Σ<em>ᵢ</em>
|<em>xₜᵢ</em> - <em>xₛᵢ</em>| + |<em>yₜᵢ</em> - <em>yₛᵢ</em>|, where <em>i</em> ranges over the number of
points, <em>xₜ</em> and <em>yₜ</em> are the coordinates of the template points, and
<em>xₛ</em> and <em>yₛ</em> are the coordinates of the stroke points.  This loss
function tells us how far the stroke is from being a “B” or whatever.</p>
<p>This may work under some circumstances, but it has some problems:</p>
<ol>
<li>
<p>The user may have written the stroke in a different <em>position</em> than
   the template, introducing <em>translational error</em> into all of the
   components of the loss function.</p>
</li>
<li>
<p>The user may have written the stroke in a different <em>orientation</em>,
   introducing <em>rotational error</em> into most of the components of the
   loss function, except those points in the center.</p>
</li>
<li>
<p>The user may have written the stroke at a different <em>size</em>,
   introducing <em>scaling error</em> into most of the components of the loss
   function, again, except for those in the center.  Moreover the
   scaling may be nonuniform — the user may have stretched the stroke
   horizontally, vertically, or diagonally without intending to do
   so — introducing <em>skewing error</em>.</p>
</li>
<li>
<p>Using the absolute difference means that an error of, say, 5 pixels
   diagonally, will count as 7 pixels; this introduces a <em>anisotropic
   bias</em> into the loss function which makes it harder to cleanly
   separate, for example, Bs from non-Bs.</p>
</li>
<li>
<p>Using the absolute difference means that an error of 20 pixels
   counts as just four times a 5-pixel error.  But, generally, if the
   user really did intend to draw a B, four 5-pixel errors are much
   more probable than a single 20-pixel error.  This, similarly, makes
   it harder to cleanly separate Bs from non-Bs.  Using the absolute
   difference represents an implied probability distribution of errors
   that is exponential.</p>
</li>
</ol>
<p>The “$1 recognizer” paper I was writing about has some solutions to
these: translate the stroke so its centroid is at the origin to
eliminate translational error, rotate it so that the start point is at
a fixed angle to eliminate rotational error, rescale it nonuniformly
horizontally and vertically so that its bounding box has size 1 in
each dimension to eliminate scaling and skewing error, and use the sum
of Euclidean distances rather than the sum of absolute differences to
eliminate anisotropic bias.  (They still have problem #5, unless I’m
imagining things and it isn’t really a problem.)  As I wrote in <a href="../notes/%25241-recognizer-diagrams.html">$1 recognizer diagrams</a>, this procedure seems unduly sensitive to
noise.</p>
<p>But it occurred to me that this was very similar to the problem of
linear regression, only with complex numbers.  If we represent each
point (<em>x</em>, <em>y</em>) as a complex number (<em>x</em> + <em>jy</em>), then rotation and
<em>uniform</em> scaling around the origin are merely multiplication by a
complex number, and translation is merely adding a complex number.  So
if the stroke is precisely a translated, uniformly scaled rotation of
the template, then there exist some complex numbers <em>m</em> and <em>b</em> such
that ∀<em>i</em>: <em>xₛᵢ</em> + <em>jyₛᵢ</em> = <em>m</em>(<em>xₜᵢ</em> + <em>jyₜᵢ</em>) + <em>b</em>.  Let’s
abbreviate the stroke point <em>i</em> as <em>sᵢ</em> = <em>xₛᵢ</em> + <em>jyₛᵢ</em> and the
template point <em>i</em> as <em>tᵢ</em> = <em>xₜᵢ</em> + <em>jyₜᵢ</em>, so that this becomes just
∀<em>i</em>: <em>sᵢ</em> = <em>mtᵢ</em> + <em>b</em>.  And if the points of the stroke are
perturbed slightly from those positions, then there exist complex
numbers <em>m</em> and <em>b</em> that give a small sum Σ<em>ᵢ</em> |<em>mtᵢ</em> + <em>b</em> - <em>sᵢ</em>|²
(the <strong>L</strong>² norm of the vector <em>mt⃗</em> + <em>b⃗</em> - <em>s⃗</em>, where all the
components of vector <em>b⃗</em> are equal); that residual sum tells us how
much error there is in the approximation, and by finding <em>m</em> and <em>b</em>
to minimize that sum, we can find something that is in some sense the
best fit.</p>
<p>This is precisely the everyday problem of linear regression, but in
the field ℂ of complex numbers, rather than the usual field ℝ of real
numbers.  The squared modulus solves problems #4 (anisotropy) and #5
(nonuniform weighting), implying a Gaussian distribution of errors,
which is probably a reasonable approximation even if not precisely
correct; and, at least in ℝ, it makes the optimization problem easy by
making it convex and differentiable in closed form.</p>
<p>If we can find the ideal rotation, scaling, and translation in this
way, we can conceivably find a better fit than the one the “$1
recognizer” finds, and maybe we can find it more efficiently, too,
especially if we can calculate it in closed form rather than using
golden-section search to heuristically approximate the optimal
rotation.  We’d still need to stretch the stroke up front to correct
for nonuniform scaling (skewing error), perhaps by calculating the
<em>x</em>–<em>y</em> covariance matrix of its points.</p>
<p>This is easier than the more general problem of trying to match up two
sets of possibly rotated, translated, and scaled points, such as star
tracking, because the correspondence between the points is provided up
front — it comes from the temporal sequence of points in the stroke.</p>
<h2>Existing work on complex linear regression</h2>
<p>Abdul Ghapor Hussin, Norli Anida Abdullah and Ibrahim Mohamed wrote a
2010 paper called <a href="http://www.ukm.my/jsm/pdf_files/SM-PDF-39-3-2010/22%20Abdul%20Ghapor.pdf">“A Complex Regression Model”</a> about using linear
regression with complex variables to predict “circular variables” such
as the direction from which waves are hitting a buoy.  I don’t really
understand their derivation.</p>
<p>Math.stackexchange.com users <a href="https://math.stackexchange.com/questions/1783397/complex-mathbb-c-least-squares-derivation">Naetmul and
hans</a>
have written up the general solution for finding a
least-squares-optimal approximate solution to the linear system
<strong>A</strong><em>x⃗</em> = <em>b⃗</em> in complex numbers.  I don’t think I can transform the
linear regression problem into this problem, because the output has
too many degrees of freedom — I’m looking for a pair (<em>m</em>, <em>b</em>)
regardless of how many input points there are, while that problem
takes the matrix <strong>A</strong> and the vector <em>b⃗</em> as given, then tries to find
the vector <em>x⃗</em> that gets you closest to it in the subspace defined by
<strong>A</strong>.  This amounts to projecting <em>b⃗</em> onto that subspace and then
figuring out where you are in it.</p>
<p>Aha!  John Cowan referred me to <a href="https://stats.stackexchange.com/questions/66088/analysis-with-complex-data-anything-different">whuber's answer on the Cross
Validated Stack Exchange</a>, where they explain that the answer is</p>
<blockquote>
<p>β^=(X<sup>∗</sup>X)<sup>−1</sup>X<sup>∗</sup>z</p>
</blockquote>
<p>along with R code to implement it and everything.  Not sure it's
rigorously proven to be the correct answer, but there are "appears to
work" arguments.</p>
<h2>Finding the least-squares solution to the univariate complex linear regression problem</h2>
<p>So we want to find argmin<sub><em>m</em>, <em>b</em> ∈ ℂ²</sub> Σ<em>ᵢ</em> |<em>mtᵢ</em> + <em>b</em> -
<em>sᵢ</em>|².  Although the complex modulus |·| isn’t differentiable
everywhere, the modulus squared |·|² is, and therefore so is the whole
sum.  So we should be able to find all of its extrema by finding where
its partial derivatives with respect to <em>m</em> and <em>b</em> are zero.  And, if
the situation is like the situation in ℝ, the derivative will only
have a single zero, and it will be the minimum; intuitively, a similar
situation ought to obtain here.</p>
<p>Let’s define Δ<em>ᵢ</em> = <em>mtᵢ</em> + <em>b</em> - <em>sᵢ</em>, so we’re trying to minimize
Σ<em>ᵢ</em> |Δ<em>ᵢ</em>|², so we set ∇[Σ<em>ᵢ</em> |Δ<em>ᵢ</em>|²] = 0, so Σ<em>ᵢ</em> ∇|Δ<em>ᵢ</em>|² = 0,
which is to say that Σ<em>ᵢ</em> ∂|Δ<em>ᵢ</em>|²/∂<em>m</em> = 0 and Σ<em>ᵢ</em> ∂|Δ<em>ᵢ</em>|²/∂<em>b</em> =
0, which are two problems we can solve separately.  ∂|Δ<em>ᵢ</em>|²/∂<em>m</em> =
(d|Δ<em>ᵢ</em>|²/dΔ<em>ᵢ</em>)(∂Δ<em>ᵢ</em>/∂<em>m</em>), and similarly for <em>b</em>, ∂|Δ<em>ᵢ</em>|²/∂<em>b</em> =
(d|Δ<em>ᵢ</em>|²/dΔ<em>ᵢ</em>)(∂Δ<em>ᵢ</em>/∂<em>b</em>).  </p><script src="../liabilities/addtoc.js"></script><div><h2>Topics</h2><ul><li><a href="../topics/programming.html">Programming</a> (286 notes)
</li><li><a href="../topics/performance.html">Performance</a> (149 notes)
</li><li><a href="../topics/algorithms.html">Algorithms</a> (123 notes)
</li><li><a href="../topics/math.html">Math</a> (78 notes)
</li><li><a href="../topics/hci.html">Human–computer interaction</a> (76 notes)
</li><li><a href="../topics/optimization.html">Mathematical optimization</a> (29 notes)
</li><li><a href="../topics/hand-computers.html">Hand computers</a> (10 notes)
</li><li><a href="../topics/linear-algebra.html">Linear algebra</a> (4 notes)
</li><li><a href="../topics/gestures.html">Gestures</a> (2 notes)
</li><li>Statistics</li><li>Regression</li></ul></div></html>