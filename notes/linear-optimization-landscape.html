<!DOCTYPE html>
<html><title>Some notes on the landscape of linear optimization software and applications ⁑ Dercuano</title><meta charset="utf-8"></meta><link href="../liabilities/style.css" rel="stylesheet"></link><meta content="width=device-width, initial-scale=1.0" name="viewport"></meta><h1>Some notes on the landscape of linear optimization software and applications</h1><div class="metadata">Kragen Javier Sitaker, 2019-08-21 (updated 2019-08-25)
(35 minutes)</div>
<p>I started auditing an “operations research” class at the UBA this
quatrimester.  It’s largely focused on linear programming, which is to
say, optimizing linear functions subject to linear constraints, but
the class also covers some other non-linear-programming topics.</p>
<p>I’m particularly interested in optimization algorithms, as I’ve said a
few times before, because they potentially raise the level of
abstraction in programming — rather than specifying how to compute
something, you just specify how to recognize if something is the thing
you wanted to compute, and then let some kind of generic solver figure
out how to solve the problem.  That’s what earned them such a
prominent place in <a href="../notes/powerful-primitives.html">More thoughts on powerful primitives for simplified computer systems architecture</a>.  And it turns out that
linear-optimization solvers, as they’re called, are the most developed
family of optimization algorithms out there.</p>
<p>Other useful overviews of the space can be found in the <a href="https://neos-guide.org/Optimization-Guide">NEOS Server
Optimization Guide</a> and the
<a href="https://en.wikibooks.org/wiki/GLPK">GLPK Wikibook</a>.</p>
<p>I’m paying as little attention as possible to proprietary software
here, but unavoidably need to mention it periodically simply because
it has played such a central role in this space.</p>
<h2>Overview of linear programming</h2>
<p>Linear programming is a particularly popular subset of mathematical
optimization because there are well-known algorithms for solving it
that scale to fairly large numbers of decision variables
(the variables the optimization algorithm
is allowed to decide on a value for to reach the optimum), and it
provides a useful approximation for many economically important
problems.  It’s so popular that sometimes it overshadows the rest of
mathematical optimization — many books purportedly about mathematical
optimization consist almost entirely of material on linear
optimization, with perhaps a short section at the end on nonlinear
optimization.</p>
<p>I’ve tended to be a bit prejudiced against linear optimization because
the real world is never perfectly linear and because I felt that
linear constraints weren’t very expressive.  But, now that I’m taking
this class, I’m starting to appreciate the astounding expressiveness of linear
programming — especially “mixed integer” linear programming
(“MIP” or occasionally “MILP”), an
extension of pure linear programming in which some of your decision
variables are constrained to integer values, which is more expressive but
enormously more difficult to solve.</p>
<p>As the lp_solve documentation summarizes it:</p>
<blockquote>
<p>The linear programming (LP) problem can be formulated as: Solve <strong>A</strong>·<em>x</em> &gt;=
 V₁, with V₂·<em>x</em> maximal. <strong>A</strong> is a matrix, <em>x</em> is a vector of (nonnegative)
 variables, V₁ is a vector called the right hand side, and V₂ is a
 vector specifying the objective function.</p>
<p>An integer linear programming (ILP) problem is an LP with the
 constraint that all the variables are integers.  In a mixed integer
 linear programming (MILP) problem, some of the variables are integer
 and others are real.</p>
</blockquote>
<p>(Some formulations additionally specify that all <em>xᵢ</em> are nonnegative,
which turns out to matter very little.)</p>
<p>There are several algorithms known for solving linear optimization
problems, of which the oldest and most-widely-used is George Dantzig’s
“simplex algorithm”, despite its worst-case exponential complexity.
Many newer solvers instead use the “interior-point method”, which has
worst-case polynomial complexity for continuous linear optimization.
(Most mature solvers support both.)  However, for mixed integer linear
programming, they all must fall back on exponential-time search
procedures, usually “branch-and-cut” procedures (also known as
“branch-and-bound”).</p>
<h2>The software landscape</h2>
<p>Typically, as an end-user, you solve linear optimization problems by
writing your problem in an “algebraic modeling language” such as GNU
MathProg or “GMPL” (a dialect of a popular proprietary modeling
language called “AMPL”) or ZIMPL; the model translator
compiles your model (including data files, which it might pull from a
database) into a standard format (such as “MPS”, “LP”, or <code>.nl</code> — see below),
generally expanding it considerably, and submits it to a “solver”.</p>
<p>Unfortunately, the most popular model translators and
solvers are all proprietary; proprietary CPLEX, now owned by IBM, is
the most popular solver, with proprietary Gurobi and
proprietary SCIP second and third, and proprietary
AMPL is the most popular model translator; there is an
excellent AMPL book by Brian Kernighan (et al., but “Kernighan” is a
sufficient recommendation) which also serves as an introduction to
linear programming.  Both CPLEX and SCIP are gratis for institutional
academic use (we are using SCIP in the class)
but not to independent researchers; I don’t know about
AMPL.  So we can forget about these.</p>
<p>On the <a href="https://neos-server.org/neos/">NEOS server</a>,
anyone can use most of this software in batch mode for free, for
optimization jobs of up to 16 megabytes of input, 3 GB of RAM, and 8
hours of run time, 
thanks to the Wisconsin Institute for Discovery at UW Madison;
the NEOS server has
AMPL, ZIMPL, CPLEX, SCIP, CLP, CBC, Gurobi, CBC, and SYMPHONY, but not
including GLPK or lp_solve, perhaps because it doesn’t consider them
“state of the art”.</p>
<p>XXX I really need a diagram!</p>
<h3>Modeler–solver interfaces (MPS, Osi, <code>.nl</code>, and LP)</h3>
<p>Often the model translator talks to the solver through a file in
some standardized file format; MPS is the most popular such format, a
punched-card-based format originally designed for IBM’s Mathematical
Programming System/360.  LP, sometimes called “CPLEX LP” because of its
origins in CPLEX, is a more-readable but less-widely-supported format;
and AMPL, a popular but proprietary algebraic modeling language
(compatible with GLPK’s MathProg),
defined its own <code>.nl</code> format, mentioned earlier, that has been
interfaced to a wide variety of solvers.</p>
<p>Linear-programming solvers have been around considerably longer than
algebraic modeling languages, so both MPS and LP are designed for
humans to write, but MPS is designed badly.</p>
<p>(There are actually two different, incompatible MPS formats: the
original fixed-field MPS format from the 1960s, and a slightly less
bletcherous “free-form” 1980s version.)</p>
<p>Here’s a small LP file that GLPK and CBC are able to accept and solve (the
optimum is z = 1, y = 0.33...).</p>
<pre><code>Maximize
 x: + 2 y + 3 z
Subject To
 a: + 2 z + 3 y &lt;= 3
Bounds
 0 &lt;= y &lt;= 4
 0 &lt;= z &lt;= 1
End
</code></pre>
<p>You can save that in <code>foo.lp</code> and solve it with GLPK by running
<code>glpsol --lp foo.lp -o foo.out</code>.  You can transate it to MPS with
<code>glpsol --check --lp foo.lp --wmps foo.mps</code>.</p>
<p>Most solvers, including GLPK’s, also have bindings that allow you to call
them from your program without preparing an input file for them to
parse; and the COIN-OR project has written a standard API called “Osi”
for doing this with many different solvers.</p>
<h3>GLPK (modeling language (GMPL/MathProg) and solver)</h3>
<p>GLPK, a C library, is the thousand-pound gorilla of linear
optimization systems; Octave is built with it, and there are bindings
to it in Java, Python, and R.  The GLPK package includes the
model translator for the popular
algebraic modeling language GNU MathProg (<em>aka</em> GMPL), and
also solvers for the revised simplex method, the primal-dual interior
point method, and the branch-and-bound method.</p>
<p>MathProg is a subset of the AMPL language mentioned above, including
nearly all the facilities of AMPL for defining optimization problems,
but without statements like <code>include</code>, <code>model</code>, and <code>data</code>.</p>
<p>It supports MPS and LP format files for both input
and output, and can even translate between them, as well as generating
them with the MathProg model translator.</p>
<p>GLPK was originally released in 2000, but has been under active
development ever since; there is a Wikibook about it, linked above in
the introduction.</p>
<p>Here’s a MathProg version of the example LP file above, with additional
directives to solve it and display the results:</p>
<pre><code>var y &gt;= 0, &lt;= 4;
var z &gt;= 0, &lt;= 1;
maximize x: 2*y + 3*z;
subject to a: 2*z &lt;= 3 - 3*y;
solve;
display x, y, z;
end;
</code></pre>
<p>If you save this as <code>min.mod</code> you can run <code>glpsol -m min.mod</code> to get
the solution, or <code>glpsol -m min.mod --wlp min.lp</code> to get a slightly
expanded version of the LP file above.  Pitfall: if it’s gzipped, like
most of the examples that come with GLPK, <code>glpsol</code> will uncompress it
to run it, but at least the version I have dies with a spurious “read
error”.</p>
<p>This example is somewhat freer-form than the LP file (there’s an
inequality with decision variables on both sides, for example), but to
really show the advantages of an algebraic modeling language like
MathProg, we need a bigger model.  For example, here’s an
integer-linear-programming sudoku solver I wrote this afternoon:</p>
<pre><code>param N default 9; set S := 1..N; param SW 'square width' default 3;
set G 'givens' default {};
param givenrow {G}; param givencol {G}; param givenchoice {G};

var X { S, S, S } binary;

maximize pleasure: 69;

rows {col in S, choice in S}: sum {row in S} X[row, col, choice] = 1;
mutex {row in S, col in S}: sum {choice in S} X[row, col, choice] = 1;
cols {row in S, choice in S}: sum {col in S} X[row, col, choice] = 1;
squares {bigrow in 0..SW-1, bigcol in 0..SW-1, choice in S}:
    sum {row in 1..SW, col in 1..SW}
        X[bigrow * SW + row, bigcol * 3 + col, choice] = 1;
givens {j in G}: X[givenrow[j], givencol[j], givenchoice[j]] = 1;
</code></pre>
<p><code>glpsol --check --wlp sudoku.lp -m sudoku.mod</code> translates this 12-line
MathProg model into a CPLEX LP file of 2000+ lines, beginning as
follows:</p>
<pre><code>\* Problem: sudoku *\

Maximize
 pleasure: 0 X(1,1,1)
\* constant term = 69 *\

Subject To
 rows(1,1): + X(1,1,1) + X(2,1,1) + X(3,1,1) + X(4,1,1) + X(5,1,1)
 + X(6,1,1) + X(7,1,1) + X(8,1,1) + X(9,1,1) = 1
 rows(1,2): + X(1,1,2) + X(2,1,2) + X(3,1,2) + X(4,1,2) + X(5,1,2)
 + X(6,1,2) + X(7,1,2) + X(8,1,2) + X(9,1,2) = 1
 rows(1,3): + X(1,1,3) + X(2,1,3) + X(3,1,3) + X(4,1,3) + X(5,1,3)
 + X(6,1,3) + X(7,1,3) + X(8,1,3) + X(9,1,3) = 1
</code></pre>
<p>There’s a SciTe-based IDE for GLPK called
<a href="http://gusek.sourceforge.net/">Gusek</a> I haven’t tried.  I’ve often
had a lot of trouble debugging my MathProg models, and I wonder if it might
help.</p>
<h3>ZIMPL (modeling language)</h3>
<p>ZIMPL, the Zuse Institute Mathematical Programming Language,
is a modeling language from the same academic research
institute, the Zuse Institute Berlin, that wrote the solver SCIP, but
ZIMPL is free software, while SCIP is not.  It’s mostly compatible
with GNU MathProg.  It started out as Thorsten Koch’s Ph.D. thesis in
2004, ZIB-Report 04-58, “Rapid Mathematical Programming”.
(“Mathematical programming” is a synonym for “mathematical
optimization”.)</p>
<p>The ZIMPL language appears to be more or less at the same level of
abstraction as MathProg, but with arguably nicer syntax.
So for example in ZIMPL you
might say</p>
<pre><code>minimize cost: 12 * x1
    + sum &lt;a&gt; in A : u[a] * y[a];
subto oneness: sum &lt;a&gt; in A : u[a] == 1;
subto nonnegative: forall &lt;a&gt; in A : y[a] &gt;= 0;
</code></pre>
<p>while in AMPL/MathProg you would say</p>
<pre><code>minimize cost: 12 * x1
    + sum {a in A} u[a] * y[a];
subject to Oneness: sum {a in A} u[a] = 1;
subject to Nonnegative {a in A}: y[a] &gt;= 0;
</code></pre>
<p>It can produce MPS and LP files, as well as something called
“Polynomial IP”.  Beware, it generates the name for its output file
from the name of the input file, and so it will overwrite any existing
file of the corresponding name.</p>
<p>Here’s a translation of the tiny model used as examples earlier into
ZIMPL:</p>
<pre><code>var y &gt;= 0;
var z &gt;= 0;
maximize x: 2*y + 3*z;
subto a: 2*z &lt;= 3 - 3*y;
subto ym: y &lt;= 4;
subto zm: z &lt;= 1;
</code></pre>
<p>If saved as <code>foo.zpl</code>, you can convert it to LP format with <code>zimpl
foo.zpl</code>, writing the output to <code>foo.lp</code>.</p>
<p>Since ZIMPL and MathProg are so similar in their capabilities, but MathProg is
bundled with the popular GLPK solver, I don’t know what the advantage
of using ZIMPL would be — except that the proprietary solver SCIP
has ZIMPL built in, and so it’s more convenient to use SCIP with ZIMPL
than with MathProg.</p>
<p>The biggest difference I’ve found so far between ZIMPL and MathProg,
actually, is that MathProg lets you specify things to do after the
solution is found, at least when you’re using the GLPK solver.  This
way you can present the solution in a more readable form.  My Sudoku
solver above, for example, has half a page of code tagged onto the end
to display an ASCII-art Sudoku board.  To do the same thing with SCIP
and ZIMPL, I ended up writing a Python script to match regexps against
the SCIP output.</p>
<h3>CMPL (modeling language)</h3>
<p><a href="https://github.com/coin-or/Cmpl">CMPL</a> is another algebraic modeling
language from the COIN-OR project, GPLv3 licensed, bundled with an IDE
called Coliop; it can use the CBC or GLPK solvers, as well as
proprietary solvers.  It also includes Java and Python interfaces,
presumably as a sort of embedded DSL.  The <a href="https://list.coin-or.org/pipermail/cmpl/2017-November/thread.html">project mailing
list</a>
seems consistently very low traffic since 2011.</p>
<p>I haven’t tried it yet, although it looks like it might address some
of my annoyances.</p>
<h3>MiniZinc (modeling language)</h3>
<p><a href="https://www.minizinc.org/">MiniZinc</a> is a constraint modeling
language that is not limited to linear constraints; it can use CBC as
a solver, as well as MinisatID, proprietary SCIP, and over a dozen
other solvers.  I haven’t tried it yet.</p>
<h3>libisl (ILP solver)</h3>
<p>The documentation says:</p>
<blockquote>
<p>isl is a library for manipulating sets and relations of integer points
 bounded by linear constraints. Supported operations on sets include
 intersection, union, set difference, emptiness check, convex hull,
 (integer) affine hull, integer projection, and computing the lexicographic
 minimum using parametric integer programming. It also includes an ILP solver
 based on generalized basis reduction.</p>
</blockquote>
<p>I have it on my laptop because GCC depends on it, apparently for a
newish loop optimization framework internal to GCC called Graphite.
It sounds scarily powerful.  Unlike the others, it’s specifically
intended for <em>integer</em> optimization, with no support for
continuous-domain optimization.</p>
<p>Needless to say, it doesn’t interoperate with the others.</p>
<h3>COIN CLP/CBC/SYMPHONY (solvers)</h3>
<p>The COIN project is an Apache project for making operations-research
results reproducible by basing them on open-source software.  It
includes the solvers CLP, CBC, and SYMPHONY; CBC and SYMPHONY require an
underlying linear-programming solver like CLP; it also defined an API
called “Osi”, the Open Solver Interface, as an alternative to the
file-format interfaces described earlier.  I haven’t yet figured out
how to use SYMPHONY yet.</p>
<p>These three solvers are the only free solvers I’ve found on the NEOS
server.</p>
<p>I’ve been able to solve minimization MPS problems with CLP using <code>clp
foo.mps</code> but solving maximization problems or seeing the actual
results requires a slightly more elaborate command:</p>
<pre><code>$ clp solar.mps -maximize -dualsimplex -solution solar.solution
</code></pre>
<p>If you apply this to an MPS file containing an integer-programming
problem like the Sudoku solver given above, you get garbage, because
CLP finds a continuous solution instead of an integer solution.  There
is a similar <code>cbc</code> executable which has an <code>-branch</code> option to avoid
this:</p>
<pre><code>$ cbc sudoku.mps -dualsimplex -branch -solution sudoku.solution
</code></pre>
<p>This finds an answer which appears to be correct, using binary
variables as Garns intended, in about half a second.  By contrast,
with GLPK, <code>glpsol --mps sudoku.mps --output sudoku.sol</code> finds a
solution in 4 seconds.  The <a href="https://www.coin-or.org/Cbc/index.html">CBC user’s
guide</a> explains in detail how
CBC supports integer variables but does not explain how to use the
<code>cbc</code> executable; Prof. Haroldo Gambini Santos wrote a short separate
<a href="https://projects.coin-or.org/CoinBinary/export/1059/OptimizationSuite/trunk/Installer/files/doc/cbcCommandLine.pdf">CBC command-line
guide</a>
in 2011; it recommends the simpler</p>
<pre><code>$ cbc sudoku.mps solve solu sudoku.solution
</code></pre>
<p>which seems to be equivalent.  Without the “solve” command (or the
lower-level <code>dualsimplex</code> and <code>branch</code> commands in the version above),
<code>cbc</code> will still be happy to write <code>sudoku.solution</code>, but it may
contain no solution, or only a solution of the linear relaxation of
the problem.</p>
<p>On a more complex problem, unfortunately, I got CBC to dump core with
this command.  The other command worked better but I think it may not
have actually given the right answer.</p>
<p>A nice feature of CBC is that if you kill it with ^C it responds by
displaying information about the process it’s made so far on the
problem (and then, as you would expect, exiting).</p>
<p>CBC also accepts input in CPLEX LP format.</p>
<p>It seems that CLP, CBC, and COIN-OR in general have stagnated somewhat
since John J. Forrest, the primary author of CLP and CBC, has retired
from IBM Research around 2005; there’s mojibake in the <a href="https://www.coin-or.org/Clp/userguide/clpuserguide.html">CLP User
Guide</a> and
the CBC User Guide, for example.  CLP and CBC are primarily intended
to be invoked via an API, but they support MPS input.</p>
<p>CLP, and perhaps COIN-OR as a whole, descends from a proprietary IBM
product called OSL, “Optimization Solutions and Library”, <a href="http://web.archive.org/web/20040203204705/http://www-306.ibm.com/software/data/bi/osl/news.html">which had
its last release in
2000</a>
and <a href="http://web.archive.org/web/20040510211307/http://www-306.ibm.com:80/software/data/bi/osl/news.html">was withdrawn in
2003</a>
with a recommendation that users switch to COIN-OR:</p>
<blockquote>
<p>For those customers considering options in the area of Optimization,
  IBM is strongly recommending the use of the open-source COIN
  solution. Much of the IBM Research Division’s efforts in
  Optimization for the past few years (along with effort from many
  others in the open-source community) have gone into developing and
  enhancing COIN. It has progressed to the point where IBM Research
  feels that COIN is equal or superior to OSL in most respects....and
  COIN is very competitive functionally and from a performance
  viewpoint with other commercially available Optimization offerings.</p>
</blockquote>
<p>According to the user guide, running <code>make unitTest</code> builds an
executable called <code>clp</code> which can be used as a standalone solver with
MPS input.  Presumably the <code>cbc</code> executable is similar.</p>
<p>(I suspect IBM acquired ILOG and thus CPLEX around 2003; the
announcement above suggests that it was maybe a bit after 2003, since
it doesn’t mention CPLEX.)</p>
<h3>DSDP (solver)</h3>
<p>The abstract of the DSDP tech report (ANL/MCS-TM-277) says:</p>
<blockquote>
<p>DSDP implements the dual-scaling algorithm for semidefinite
  programming. The source code if[sic] this interior-point solver,
  written entirely in ANSI C, is freely available. The solver can be
  used as a subroutine library, as a function within the MATLAB
  environment, or as an executable that reads and writes to
  files. Initiated in 1997, DSDP has developed into an efficient and
  robust general purpose solver for semidefinite programming. Although
  the solver is written with semidefinite programming in mind, it can
  also be used for linear programming and other constraint cones.</p>
</blockquote>
<p>Apparently <a href="https://en.wikipedia.org/wiki/Semidefinite_programming">semidefinite
programming</a>
is some kind of generalization of linear programming:</p>
<blockquote>
<p>All linear programs can be expressed as SDPs, and via hierarchies of
  SDPs the solutions of polynomial optimization problems can be
  approximated. ... A linear programming problem is one in which we
  wish to maximize or minimize a linear objective function of real
  variables over a polytope. In semidefinite programming, we instead
  use real-valued vectors and are allowed to take the dot product of
  vectors; nonnegativity constraints on real variables in LP (linear
  programming) are replaced by semidefiniteness constraints on matrix
  variables in SDP (semidefinite programming).</p>
</blockquote>
<p>The <a href="https://web.stanford.edu/~boyd/papers/sdp.html">paper that introduced semidefinite
programming</a> gives
further context.</p>
<blockquote>
<p>Semidefinite programming unifies several standard problems (eg,
  linear and quadratic programming) and finds many applications in
  engineering. Although semidefinite programs are much more general
  than linear programs, they are just as easy to solve. Most
  interior-point methods for linear programming have been generalized
  to semidefinite programs.</p>
</blockquote>
<p>This all sounds great, but, to me at least, it is not at all apparent
how to use the <code>dsdp5</code> command to solve a linear program.  The
documentation is all for the library’s C API, and so are the examples.</p>
<h3>NLopt (solver)</h3>
<p>NLopt is a <em>non</em>linear optimization library, so it probably doesn’t
really belong in this list, but in theory it should be able to solve
linear optimization problems too, as a special case; it will be
interesting to compare it.  It has bindings in C, C++, Fortran,
Octave, Python, Guile, and R.</p>
<h3>Octave (programming environment)</h3>
<p>Although Octave is mostly a matrix computation system, it includes
support for linear programming, as well as quadratic programming,
general nonlinear programming, and linear least-squares solution of
matrices.</p>
<h3>Sagemath (programming environment with modeling embedded DSL)</h3>
<p>The Sage math programming environment has <a href="http://doc.sagemath.org/html/en/reference/numerical/mip.html">a mixed integer linear
programming
module</a>
which provides an embedded DSL in the Sage environment for
constructing MILP optimization problems.  It looks a little clumsier
than MathProg.  It can use GLPK, CBC, CVXOPT, PPL, or some proprietary
solvers.</p>
<p>It <em>also</em> has a semidefinite programming module, using CVXOPT as the
default (and by default only supported) solver.</p>
<p>I haven’t tried it.</p>
<h3>Parma Polyhedra Library “PPL” (solver)</h3>
<p>The GPL C++ <a href="https://www.bugseng.com/parma-polyhedra-library">Parma Polyhedra
Library</a> provides a
kind of generalization of mixed integer linear programming that I
don’t fully understand, especially suited for static analysis of
hardware systems, with APIs in C, Java, OCaml, and Prolog.  I haven’t
tried it.  The comprehensive user guide says a lot of things like
this:</p>
<blockquote>
<p>Note: The affine dimension <em>k</em> &lt;= <em>n</em> of an NNC polyhedron P in Pn
  must not be confused with the space dimension <em>n</em> of P, which is the
  dimension of the enclosing vector space R^n.</p>
</blockquote>
<h3>CVXOPT (modeling embedded DSL and solver)</h3>
<p>CVXOPT is a GPL3+ library for Python under active development since
2004; it is intended for convex optimization but provides a lot of
general numerical functionality, including interfaces to GLPK, plus
its own solvers for linear and quadratic optimization (under the
rubric “cone programming”), as well as nonlinear optimization and
semidefinite programming.  I haven’t tried it but I assume it is
suitable for writing your models in too.</p>
<h3>Pyomo (modeling embedded DSL)</h3>
<p>Pyomo, formerly “Coopr”, is an algebraic modeling language like ZIMPL
or MathProg, but realized as an embedded DSL in Python, or perhaps more
accurately, a Python API, with the concepts closely matching those of
ZIMPL or MathProg.  It supports linear optimization and also quadratic
optimization, general nonlinear optimization, etc.  It’s able to read
model parameters from data files in MathProg format, but not to read MathProg
models.</p>
<p>Pyomo can use GLPK, CBC, or some undocumented set of other solvers.</p>
<h3>FLOPC++ (modeling embedded DSL)</h3>
<p>FLOPC++ — another part of the COIN-OR project — is, similarly, an
algebraic modeling language realized as an embedded DSL in C++.  I
haven’t tried it but it looks like roughly the same level of pain as
Pyomo.  It can at least use CBC.</p>
<h3>Google OR-Tools (solvers and embedded DSLs in multiple languages)</h3>
<p>Google has a set of free-software “operations research tools” called
OR-Tools, including a linear optimizer called
<a href="https://developers.google.com/optimization/lp/glop">Glop</a>, licensed
under the Apache license 2.0, with bindings in Python, C++, Java, and
C#.  Glop doesn’t seem to support MIP, but the bundle also includes a
constraint solver and a SAT solver, as well as interfaces to external
MIP solvers (they recommend SCIP).  I haven’t tried any of them yet.</p>
<h3>lp_solve (solver)</h3>
<p>lp_solve is a simplex-method solver, using branch-and-bound for mixed
integer programming, which supports the MPS file format.  Early
versions were proprietary, but recent versions are free software.</p>
<p>I had some trouble getting it to interoperate with GLPK or ZIMPL.  I
<em>did</em> eventually get it to read an MPS file generated by ZIMPL, and I
think I know why the GLPK-generated version was failing; my example
problem (given in MathProg and LP format above) is a maximization problem,
not a minimization problem.  Upon translating it to MPS with <code>zimpl -t
mps min.zpl</code> (see below), ZIMPL issued the following warning:</p>
<pre><code>--- Warning: Objective function inverted to make
             minimization problem for MPS output
</code></pre>
<p>And then lp_solve was able to solve it with <code>lp_solve -mps min.mps</code>,
and indeed the value of the objective function was -3.66...7 rather
than 3.66...7.  So I guess that the problem was that MPS doesn’t have
a way to say you’re trying to maximize the objective function, not
minimize it.</p>
<p>With this in mind, I was then able to solve the free-form MPS file I
had generated with <code>glpsol -m min.mod --wfreemps min.fmps</code> by using
<code>lp_solve -max -fmps min.fmps</code>, <code>-max</code> being an <code>lp_solve</code> flag to
override this.  GLPK can also generate fixed MPS files lp_solve can
read with its <code>-mps</code> flag.</p>
<p>lp_solve has its own “LP format” which is not the same as the CPLEX
LP format used by GLPK, ZIMPL, CLP, and CBC, leading to much confusion on my
part.  The documentation does explain this:</p>
<blockquote>
<p>The lp-format is lpsolves [sic] native format to read and write lp
  models. Note that this format is not the same as the CPLEX LP format
  (see CPLEX lp files) or the Xpress LP format (see Xpress lp files)</p>
</blockquote>
<p>I was able to translate the file from MPS to this incompatible LP
format with <code>lp_solve -max -fmps min.fmps -wlp min.lps.lp</code>:</p>
<pre><code>/* min */

/* Objective function */
max: +2 y +3 z;

/* Constraints */
a: +3 y +2 z &lt;= 3;

/* Variable bounds */
y &lt;= 4;
z &lt;= 1;
</code></pre>
<p>This can be reduced to the following, a format which is quite
reasonable for writing by hand, and still work as input to lp_solve:</p>
<pre><code>max: +2 y +3 z;
a: +3 y +2 z &lt;= 3;
y &lt;= 4;
z &lt;= 1;
</code></pre>
<p>Unfortunately nothing but lp_solve supports this format.  (But I
think it can translate it to MPS format.)</p>
<p>In theory lp_solve supports CPLEX LP format with the flags <code>-rxli
xli_CPLEX</code> to read or <code>-wxli xli_CPLEX</code> to write, but I think the
Debian package failed to include the relevant module, so this doesn’t
work.  (If you really needed to do this, you could use GLPK to
translate from LP to MPS so lp_solve can handle
it.)</p>
<p>lp_solve even supposedly has an “xli” to read MathProg, presumably linked
with GLPK.</p>
<p>lp_solve seems to be much weaker than GLPK’s solvers; I have an
underconstrained 9x9 Sudoku puzzle, using the above solver, here that
GLPK can solve in 4.1 seconds, but lp_solve chewed on it
for 22 hours without solving it before I killed it.  The two projects started
about the same time, but GLPK has seen continued development.  I
suspect that any current interest in lp_solve is largely because GLPK
is GPL, so you can’t build proprietary software with it, while
lp_solve is LGPL, so you can.</p>
<p>Like GLPK, lp_solve also has an API as well as file formats — in C,
Java, and Free Pascal.</p>
<h2>Hacks to expand what you can do with a linear optimizer</h2>
<p>The description from lp_solve makes linear optimization sound really
weak and limiting:</p>
<blockquote>
<p>Solve <strong>A</strong><em>x⃗</em> ≥ V⃗₁, with V⃗₂·<em>x⃗</em> maximal.</p>
</blockquote>
<p>And, in some ways, it is; but as I’m learning in this class, there are
standard hacks for turning a much larger variety of problems into
linear problems.  Some of these are done for you automatically by
model translators for algebraic modeling languages, while others
aren’t.</p>
<h3>Minimizing vs. maximizing</h3>
<p>The simplest hack is switching between minimizing and maximizing.  The
lp_solve definition above might suggest that you can’t minimize
things, but of course when <em>x⃗</em> is chosen to make V⃗₂·<em>x⃗</em> minimal, then
(-1 V⃗₂) · <em>x⃗</em> is maximized.  Moreover, its maximal value is
precisely the opposite of the minimal value for V⃗₂·<em>x⃗</em>; so if your
solver wants to maximize, you can just flip the sign on its objective
vector and the objective result, and you can minimize.</p>
<p>As mentioned earlier, ZIMPL will do this when generating an MPS file,
since apparently MPS can’t express whether you want to maximize or
minimize your objective function, and so given just an MPS file,
GLPK, lp_solve, CLP, and CBC assume you want to
minimize.</p>
<h3>Mixing ≥ with ≤</h3>
<p>Similarly, <strong>A</strong><em>x⃗</em> ≥ V⃗₁ suggests that all your constraints must
have decision variables on the left side of a “≥”.  But that’s silly;
the constraint 2<em>y</em> ≥ 4, for example, is equivalent to -2<em>y</em> ≤ -4.
So by flipping the signs on a row of the matrix and the corresponding
limit, you can effectively get both ≤ and ≥ constraints in the same
matrix.  And that’s how you can get criteria like 1 ≤ 2<em>y</em> + 3<em>z</em>
≤ 2.</p>
<p>Indeed, even the CPLEX LP format supports this directly for decision
variables:</p>
<pre><code>Bounds
 0 &lt;= y &lt;= 4
 0 &lt;= z &lt;= 1
</code></pre>
<h3>Decision variables on both sides</h3>
<p>Similarly, if you have a criterion like 2<em>y</em> ≤ 3<em>z</em> + 1, you might
think that doesn’t fit into the <strong>A</strong><em>x⃗</em> ≥ V⃗₁ mold.  But all you
have to do is subtract 3<em>z</em> from both sides — 2<em>y</em> - 3<em>z</em> ≤
1 — and Bob’s your auntie!</p>
<h3>Equality constraints</h3>
<p>Similarly, if you have an <em>equality</em> criterion like 3<em>p</em> = 4<em>q</em> -
2<em>r</em>, you can convert it into two <em>inequalities</em> — 3<em>p</em> ≤ 4<em>q</em> -
2<em>r</em> and 3<em>p</em> ≥ 4<em>q</em> - 2<em>r</em> — which can only be simultaneously
true when strict equality is satisfied.  This reduces the
dimensionality of your feasible region below the full dimensionality
of the decision-variable space, though that’s not a problem for the
simplex method.</p>
<h3>Binary gates with big <em>M</em></h3>
<p>A common nonlinearity that can be handled fairly easily by integer
linear programming is a sort of “something-or-nothing” criterion; for
example, with solar panels, the amount of power you can produce at a
site (and thus money you can make) is proportional to the amount of
panel area installed at the site, but to produce any power at all at
the site, you need to run power lines to the site, which has some
fixed cost.</p>
<p>If you have some upper limit <em>M</em> on the power you can produce, you can
multiply that upper limit by a binary variable <em>W</em> that encodes
whether or not a site is active; for example, in MathProg notation:</p>
<pre><code>param M {s in Sites} := available_area[s] * power_per_area;
var W {s in Sites}, binary;
subject to power_installation {s in Sites}:
    0 &lt;= panels_installed[s] &lt;= W[s] * M[s];
</code></pre>
<p>When <em>Wₛ</em> is 0, this forces <code>panels_installed[s]</code> to also be 0; when
<em>Wₛ</em> is 1, <code>panels_installed[s]</code> can have any value at all, as long as
it’s less than <em>M</em>, which was chosen to be large enough not to be a
restriction in practice.  (Solvers can suffer precision problems if
<em>M</em> is too many orders of magnitude larger than the actual values of
the variable, though.)</p>
<p>Then, you can use this variable <em>W</em> in your objective function to take
into account the installation cost.</p>
<p>(I mean “gate” in the sense of something that can block something else
from happening, not in the sense of a NAND gate.)</p>
<h3>Absolute values</h3>
<p>A much cooler trick that doesn’t even require dropping back to integer
linear programming is encoding absolute values.  Suppose one of the
terms of an objective function you’re trying to <em>minimize</em> is the
absolute value of some linear function <em>f</em> of your decision variables,
|<em>f</em>(<em>x⃗</em>)|.  That’s not linear, or even differentiable, because it
changes direction abruptly when <em>f</em>(<em>x⃗</em>) = 0, so you can’t solve it
directly with a linear solver.  What you can do is replace |<em>f</em>(<em>x⃗</em>)|
with a new variable <em>t</em> which is constrained to be greater than or
equal to |<em>f</em>(<em>x⃗</em>)| using two new constraints: <em>t</em> ≥ <em>f</em>(<em>x⃗</em>) and
<em>t</em> ≥ -<em>f</em>(<em>x⃗</em>).  This ensures that <em>t</em> ≥ |<em>f</em>(<em>x⃗</em>)|, which, in
conjunction with the minimization criterion, forces <em>t</em> = |<em>f</em>(<em>x⃗</em>)|,
and again, Bob’s your auntie!</p>
<p>When you’re trying to <em>maximize</em> a sum including |<em>f</em>(<em>x⃗</em>)|, which is
a more unusual thing to do, this becomes trickier; you must drop back
to mixed integer programming and introduce a binary variable <em>W</em> and
maximum <em>M</em> bound and use a variant of the binary-gate trick to
prevent <em>t</em> from ever being strictly greater than <em>f</em>(<em>x⃗</em>), with <em>t</em> -
<em>f</em>(<em>x⃗</em>) ≤ <em>WM</em> and <em>t</em> - (-<em>f</em>(<em>x⃗</em>)) ≤ (1-<em>W</em>)<em>M</em>.</p>
<h3>Mutual exclusion</h3>
<p>As demonstrated in the Sudoku example above, you can use a simple sum
condition to require that precisely one of a set of options be chosen:
<em>W</em>1 + <em>W</em>2 + <em>W</em>3 = 1.</p>
<h3>Piecewise-linear functions</h3>
<p>Mutual exclusion allows you to use piecewise-linear functions in your
linear constraints and objectives.  You can’t do this in the obvious
way — <em>W</em>₁ <em>f</em>₁(<em>x</em>) + <em>W</em>₂ <em>f</em>₂(<em>x</em>) + <em>W</em>₃ <em>f</em>₃(<em>x</em>), with the
<em>fᵢ</em> being the linear pieces — because it wouldn’t be linear.  But
you can introduce new “abscissa” variables <em>z</em>₁, <em>z</em>₂, <em>z</em>₃ that are
gated by the <em>Wᵢ</em> and restricted to their own regions: 3<em>W</em>₁ ≤ <em>z</em>₁
≤ 6<em>W</em>₁, for example, restricts <em>z</em>₁ to [3, 6] when <em>W</em>₁ = 1, and 0
otherwise.  Then <em>f</em>₁(<em>z</em>₁) + <em>f</em>₂(<em>z</em>₂) + <em>f</em>₃(<em>z</em>₃) gives you the
piecewise-linear function you wanted, and <em>z</em>₁ + <em>z</em>₂ + <em>z</em>₃ gives you
its abscissa, which perhaps you want to constrain to be equal to <em>x</em>.</p>
<p>This is a variant of the binary-gate trick that requires no big <em>M</em>.</p>
<p>(I've written it here with single scalars <em>zᵢ</em>, but you can extend it
to piecewise-linear functions of multiple variables in a few different
ways.)</p>
<p>This has the disadvantage that if your piecewise-linear function is
discontinuous, it ceases to be a <em>function</em> at the discontinuities; it
can take two different values.</p>
<p>(I think there’s also a way to make a <em>convex downwards, continuous</em>
piecewise-linear function out of absolute values without involving
integer linear programming, in the case where you’re minimizing, but I
haven’t seen it worked out yet.)</p>
<h3>Ratios between linear forms where the denominator has known sign</h3><script src="../liabilities/addtoc.js"></script><div><h2>Topics</h2><ul><li><a href="../topics/programming.html">Programming</a> (286 notes)
</li><li><a href="../topics/math.html">Math</a> (78 notes)
</li><li><a href="../topics/optimization.html">Mathematical optimization</a> (29 notes)
</li><li>Linear optimization</li><li>Libraries</li></ul></div></html>