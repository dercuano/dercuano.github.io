<!DOCTYPE html>
<html><title>Compact namespace sharing ⁑ Dercuano</title><meta charset="utf-8"></meta><link href="../liabilities/style.css" rel="stylesheet"></link><meta content="width=device-width, initial-scale=1.0" name="viewport"></meta><h1>Compact namespace sharing</h1><div class="metadata">Kragen Javier Sitaker, 2016-07-25
(7 minutes)</div><p>Smalltalk bytecode is super compact, partly because it doesn’t include
literal constants, addresses, and method selectors in the bytecode
itself; instead they’re stored in a separate per-method vector and
referred to by index.  Java bytecode uses the same approach, but the
result is that even though the bytecode itself is very compact, the
.class files can easily be far from compact, more like huge and
bloated.</p>
<p>You could imagine sharing, say, a single constant pool over an entire
static program, just as some Smalltalks have shared a single object
table over an entire dynamic program.  Then, each constant would only
be listed once in the pool.  But it would be a large pool, so an
arbitrary index into it could occupy many bits, such as 16 or even
more.  By contrast, Smalltalk bytecodes are only 8 bits, and typically
only 3–5 of these are occupied in indexing into such a pool.  This
static locality of reference gives us potentially very considerable
compression, but at the cost of duplication among the “dictionaries”
used for this compression.</p>
<p>This is not the only possible compression mechanism for this
information — you could also imagine things like move-to-front coding,
and Smalltalk also has a fixed dictionary of special constants
accessed by a separate opcode — but if that's the one we’re using, we
can consider how to optimize it better.</p>
<p>Function calls and returns are a kind of context switch: after the
transfer of control, the variables and constants of interest to the
new code are no longer the same as those of interest to the old code,
much as in a switch between threads or processes.  In 1973, Carl
Hewitt, Peter Bishop, and Richard Steiger suggested conceptualizing
each function’s activation record as a separate “actor”, like a
process; “actors [or, if you will, virtual processors, activation
frames, …] … it is impossible to determine whether a given object is
“really” represented as…a hash table, a function, or a process,” they
said.</p>
<p>Context switches can have widely varying costs.  Context switches that
happen more often have a proportionally higher total cost, whatever
other factors may be involved; and context switches that overwrite
more state also have a proportionally higher cost.  In the context of
function calls and returns, a return that need only restore a saved PC
and SP is inexpensive; a return that must restore %eip, %esp, %ebx,
%ebp, %esi, and %edi is more expensive; and a return that must
additionally restore floating-point registers, vector registers, and
some kind of dynamic exception handling context, is more expensive
still.</p>
<p>However, the alternative to overwriting more state is typically adding
another level of indirection, and thus cost, to things that happen
when you’re not context switching.  Bernie Greenberg’s <a href="http://multicians.org/mepap.html">Multics Emacs
paper</a> explains his choice of a heavyweight context-switch mechanism
for switching buffers in Multics Emacs as follows:</p>
<blockquote>
<p>The implementation of multiple buffers was viewed as a task of
multiplexing the extant function of the editor over several
buffers. The buffer being edited is defined by about two dozen Lisp
variables of the basic editor, identifying the current Editorline,
its current (open/closed) state, the first and last Editorlines of
the buffer, the list of marks, and so forth. Switching buffers
(i.e., switching the attention of the editor, as the user sees it)
need consist only of switching the values of all of these
variables. Neither the interactive driver nor the redisplay need be
cognizant of the existence of multiple buffers; the redisplay will
simply find that a different "current Editorline" exists if buffers
are switched between calls to it. What is more, the only functions
in the basic editor that have to be aware of the existence of
multiple buffers are those that deal with many buffers, switch them,
etc. All other code simply references the buffer state variables,
and operates upon the current buffer.</p>
<p>The function in the basic editor which implements the command that
switches buffers does so by saving up the values of all of the
relevant Lisp variables, that define the buffer, and placing a saved
image (a list of their values) as a property of the Lisp symbol
whose name is the current buffer's. The similarly saved list of the
target buffer's is retrieved, and the contained values of the buffer
state variables instated. A new buffer is created simply by
replacing the "instatement" step with initialization of the state
variables to default values for an empty buffer. Buffer destruction
is accomplished simply by removing the saved state embedded as a
property: all pointers to the buffer will vanish thereby, and the
MacLisp garbage collector will take care of the rest.</p>
<p>The alternate approach to multiple buffers would have been to have
the buffer state variables referenced indirectly through some
pointer which is simply replaced to change buffers. This approach,
in spite of not being feasible in Lisp, is less desirable than the
current approach, for it distributes cost at variable reference
time, not buffer-switching time, and the former is much more common.</p>
</blockquote>
<p>This is also the reason shallow binding is usually preferred to deep
binding in implementing dynamically-scoped languages: shallow binding
makes function calls expensive, but variable access cheap.</p>
<p>So suppose that, when we enter a function, we load a whole passel of
values into the registers of the machine, virtual or otherwise;
there’s a passel pointer before the beginning of the function code,
and it is used to initialize the registers.  Some of these registers
may contain “constants” that we want to be able to use, others contain
initial values for local variables, and others still may be instance
variables of an object, which will need to be saved back to the object
when the function is done.</p>
<p>The wide cache memory buses featured on modern high-performance
processors can transfer 64 bytes at a time; wide 128-bit, 256-bit, and
512-bit (16-, 32-, and 64-byte) SIMD registers can be loaded in a
single cache access (right?).  Hardware designed slightly differently
could</p>
<p>fixed levels of hierarchy</p>
<p>wide memory</p>
<p>message queues</p><script src="../liabilities/addtoc.js"></script><div><h2>Topics</h2><ul><li><a href="../topics/programming.html">Programming</a> (286 notes)
</li><li><a href="../topics/performance.html">Performance</a> (149 notes)
</li><li><a href="../topics/history.html">History</a> (71 notes)
</li><li><a href="../topics/small-is-beautiful.html">Small is beautiful</a> (40 notes)
</li><li><a href="../topics/instruction-sets.html">Instruction sets</a> (40 notes)
</li><li><a href="../topics/compression.html">Compression</a> (28 notes)
</li><li><a href="../topics/smalltalk.html">Smalltalk</a> (12 notes)
</li></ul></div></html>