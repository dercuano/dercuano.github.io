<!DOCTYPE html>
<html><title>A principled rethinking of array languages like APL ⁑ Dercuano</title><meta charset="utf-8"></meta><link href="../liabilities/style.css" rel="stylesheet"></link><meta content="width=device-width, initial-scale=1.0" name="viewport"></meta><h1>A principled rethinking of array languages like APL</h1><div class="metadata">Kragen Javier Sitaker, 2015-05-16 (updated 2019-09-30)
(31 minutes)</div><p>What accounts for the power and convenience of array-processing
languages like Matlab, R, and APL?  Can we get it without having to
pay the high readability and bugginess costs associated with these
languages?</p>
<p>I feel that I may finally have an account of what’s going on here, and
not only can we get better error-checking out of it, we can get a
language that is more expressive (in the sense of more power in less
tokens), more readable, and more efficient than traditional
array-processing languages.</p>
<p>Though I think the present note is self-contained, the initial
development of the idea was in <a href="../notes/matrix-multiply.html">Index set inference or domain inference for programming with indexed families</a> in probably 2007
or 2008.</p>
<h2>The basic idea</h2>
<p>A variable in a computer program has a value that varies.  Sometimes
when a piece of code runs, the variable will have one value; other times, it
will have another value.  (In many languages it can change even during
the same run, but that is not relevant to what I am considering here.)</p>
<p>These values are functions of some arbitrary set of inputs, often
implicit in their context.  In image processing, a brightness value
might vary by X, Y, color channel, and frame number; in statistical
processing, an elapsed-time measurement might vary by trial number; in
rendering, a pixel color might vary by shading algorithm, camera
position, and the states of all the objects in the input scene.  It
can be hard to tell what they depend on, and indeed assuming that a
variable is constant relative to a given input, when in fact it ought
to vary, is a common source of bugs.  (You could argue that this is
the source of the difficulty of caching.)</p>
<p>We could think of these inputs as being dimensions in a
many-dimensional space, and each point in this space as being a
possible universe — perhaps a very small possible universe with only a
few variables in it, but a universe nonetheless.</p>
<p>Array languages allow us to reify this space in runtime variables, and
thus to write programs that act across entire subspaces of it, rather
than the pointwise and one-dimensional approach taken by ALGOL-family
languages like C or Java.</p>
<h2>Some examples from ray-tracing</h2>
<p>Consider this C code, from <a href="http://canonical.org/~kragen/sw/aspmisc/my-very-first-raytracer">My Very First Raytracer</a>:</p>
<pre><code>static color
pixel_color(world here, int ww, int hh, int xx, int yy)
{
  vec pv = { (double)xx/ww - 0.5, (double)yy/hh - 0.5, 1 };
  ray rr = { {0}, normalize(pv) };
  return trace(here, rr, 1.0);
}
</code></pre>
<p>This code is invoked a number of times with the same <code>world</code>, <code>ww</code>,
and <code>hh</code> variables, but with varying values for <code>xx</code> and <code>yy</code>; but you
can’t tell that from looking at it.  Similarly, when it invokes
<code>trace</code>, it invokes it many times with the same <code>here</code> value,
different <code>rr</code> values, and the same <code>importance</code> value of 1.0.  But it
needs to be written as a function, or at least a loop, to allow this
flexibility in <code>xx</code> and <code>yy</code>.</p>
<p>In a sense, in this code, <code>xx</code> represents not a single integer, but an
entire plethora of possible values, maybe even an infinite series of
values; it’s not a scalar, but rather a function of which pixel we’re
looking at.  When we write the division of <code>xx</code> by <code>ww</code>, we are not
writing the division of a single floating-point number by a single
integer, but rather all the floating-point numbers <code>xx</code> will ever
convert to in the lifetime of humanity, by all the possible image
widths <code>ww</code> will ever contain in the lifetime of humanity.  Or, if we
limit our perspective to a single execution, that division instruction
will eventually be used to divide all of the horizontal image pixel
coordinates by the image width — redundantly, many times, in fact,
once for each line; so it’s a machine instruction that implicitly
represents a vector operation.</p>
<p>But this is a rather unusual hermeneutics of the C and machine code.</p>
<p>The C code enforces a particular order of evaluation: it is not
capable of beginning to evaluate a second call to <code>trace()</code> until the
first one is done, and no way to evaluate a second call to
<code>pixel_color</code> until the first one is done.  But this may not be the
most efficient way to find out what color the pixels should be.</p>
<p>The C code, you’ll notice, also has a fair bit of syntactic overhead
associated with allowing these variables to vary; they have to be
declared as parameters.  What if, instead, we were programming in a
language where these variables explicitly, in the source text and in
memory at run time, represented an entire vector of possibilities — a
sort of more principled APL?  Maybe we could write it something more
like this:</p>
<pre><code>pixel_color = trace(here,
                    ray(vec(0,0,0), normalize(vec(xx/ww-.5, yy/hh-.5, 1))),
                    1.0);
</code></pre>
<p>(We’re also getting some brevity here by not having to name the
temporary structs.)</p>
<p>A language with an APL-like evaluation strategy could figure out that
<code>xx</code> and <code>yy</code> vary independently, while <code>ww</code> and <code>hh</code> don’t vary at
all, and so generate a ρ<code>xx</code>×ρ<code>yy</code> space of possibilities for the
<code>vec</code>s that we’re normalizing, where ρ is the APL operator that gives
the shape of a matrix.  (More detail on how to do this is in the next
section.)</p>
<p>I think that’s the ultimate philosophical justification for APL’s
conformability and broadcasting rules; if you’re ray-tracing 640
columns and 480 rows of pixels, for example, then a value that is
constant for all those pixels is merely a scalar; a value that varies
by column but not by row will be a 640-element vector; a value that
varies by row but not by column will be a 480-element vector; and a
value that varies by pixel will necessarily be a 640×480 matrix.  So
it makes sense to divide <code>xx</code> by <code>ww</code>, or <code>yy</code> by <code>hh</code>, but doing
anything with the two of them together requires an outer-product
operation (which in APL is explicit) or reducing one or the other of
those axes of variation into being some kind of dummy variable, like
an index of summation or whatnot.</p>
<p>But APL of course can’t tell that your 3-element vector representing
the X-coordinates of the three spheres in your scene isn’t really
compatibly dimensioned to a three-element vector that represents the
X, Y, and Z coordinates of the camera, say.  Array-dimensions typing
is weak typing, much like the currently fashionable approach of typing
everything as a string.  And this is why outer products are
necessarily explicit in APL, while I think a more principled
array-processing language could infer most of them.</p>
<p>Here’s another example, from the same program:</p>
<pre><code>static vec
scale(vec vv, sc c) { vec rv = { vv.x*c, vv.y*c, vv.z*c }; return rv; }

static ray
reflect(ray rr, vec start, vec normal)
{
  // Project ray direction onto normal
  vec proj = scale(normal, dot(rr.dir, normal));
  // Subtract that off twice to move the ray to the other side of surface
  vec reflected_dir = sub(rr.dir, scale(proj, 2));
  ray rv = { add(start, scale(reflected_dir, 0.001)), reflected_dir };
  return rv;
}
</code></pre>
<p>(The .001 fudge factor there is to keep the reflected ray from hitting
the same surface again from the inside due to rounding errors.)</p>
<p>The <code>scale</code> function here obviously only exists because C is not an
array-processing language.  Or does it?  If we were trying to write a
<code>reflect</code> that handled many normals at once in arrays, it wouldn’t be
totally insane to use three separate arrays of X, Y, and Z components
of the normals.  Taking just the first line of <code>reflect</code> and
translating it into C written as if it were Fortran:</p>
<pre><code>static void
scale(sc vx[], sc vy[], sc vz[], sc c[],
      sc vox[], sc voy[], sc voz[], int n)
{
  for (int ii = 0; ii &lt; n; ii++) {
    vox[ii] = vx[ii] * c;
    voy[ii] = vy[ii] * c;
    voz[ii] = vz[ii] * c;
  }
}

static void
proj(sc vx[], sc vy[], sc vz[], sc dx, sc dy, sc dz,
     sc vox[], sc voy[], sc voz[], int n)
{
  sc dots[n];
  for (int ii = 0; ii &lt; n; ii++) {
    dots[ii] = vx[ii] * dx + vy[ii] * dy + vz[ii] * dz;
  }
  scale(vx, vy, vz, dots, vox, voy, voz, n);
}
</code></pre>
<p>You may not agree yourself that this would not be totally insane, but
hopefully you can agree that this is a way to do this that Fortran
programmers would think was not totally insane.  Also you can see that
an enormous benefit of APL over Fortran for this kind of thing is that
you have at least some hope of changing your mind about whether it was
the rays or the normals or both that you wanted to have more than one
of, because making the loops and indexing implicit there means that
you don’t have to change the code to index into a <code>dx</code> array and maybe
not index into a <code>vx</code> array.</p>
<p>(Also there are probably some combinations of dimensions and CPU
models for which the more predictable memory access of this version
would actually make it faster despite its smaller ratio of computation
to memory locations accessed.  And obviously if your loops are
implicit and your non-implicit operations are subject to
interpretation overhead, as in Numpy or normal APL implementations,
the array approach is going to be hugely faster.)</p>
<p>Coordinates in three-space, though, are definitely the kind of thing
that it’s reasonable to think of as numbering from 0 to 2 or from 1
to 3, rather than being unrelated attributes of the same thing.  Then
you might want your array of normals here to be represented as an n×3
array rather than three arrays of n or an array of n structs with
three fields.  And then things like the <code>scale</code> function fall away
entirely, but you need some way to specify which dimension you’re
summing over when you compute that dot product.  APL <code>+/</code> has a
default of operating over the <em>last</em> dimension, and the option of
specifying a different dimension by its numerical index, as in
<code>+/[1]</code>.</p>
<p>This seems ad-hoc and unreadable to me, like much of APL.  But if you
have named your axes of variation, and one of them is the XYZ
distinction, then you could very reasonably say <code>XYZ.sum()</code> or
<code>+/[XYZ]</code>, and it would be clear, turning the XYZ variation into a
dummy variable; if you applied it to some kind of aggregate with more
than one XYZ distinction (introduced with an explicit outer-product
operator) or no XYZ distinction, you would get an error.</p>
<p>And then you could write</p>
<pre><code>proj = normal * XYZ.sum(raydir * normal)
</code></pre>
<p>instead of the 20 lines of crap above, and furthermore keep that
abstract over whether you have a single normal and many ray
directions, a single ray direction and many normals, many normals of
which each corresponds to a different ray direction, or even (what APL
could never do implicitly) many ray directions and many normals, of
which we implicitly want the Cartesian product.</p>
<p>And then maybe you could write the whole function like this:</p>
<pre><code>proj = normal * XYZ.sum(raydir * normal)
reflected_dir = raydir - 2 * proj
reflect = ray(start + reflected_dir * 0.001, reflected_dir)
</code></pre>
<p>When it comes to implicitly broadcasting operations over different
dimensions, C is equivalently succinct to an array language — modulo
the data typing and abstraction overhead that it requires in order to
give you variables at all.  But because C values are only implicitly,
in an esoteric hermeneutics, vectors or universes of possibilities, it
is difficult to write something like the <code>XYZ.sum</code> function above;
instead we are reduced to writing explicit loops, or as in this case,
explicitly textually repetitive code.</p>
<h2>Getting more rigorous: a functional semantics with implicit arguments</h2>
<p>Okay, “rigorous” and “semantics” may be an exaggeration.  But I’ll try
to at least outline a rigorous semantics here.</p>
<p>Suppose that, instead of considering variables such as <code>normal</code> to
hold scalars, vectors, or matrices of some finite size, we instead
consider them to hold computable functions, but functions whose domain
is not necessarily known or finite.  This is not a big leap: we can
consider the vector [6 832 4] as a function f over a domain of three
integers: it returns 6 when invoked as f(0), 832 when invoked as f(1),
or 4 when invoked as f(2).</p>
<p>In this interpretation, we lift the usual arithmetic operators to
operate over functions of one argument in the usual way: <code>*</code>, for
example, is the function we would usually write in the λ-calculus as
<code>λf.λg.λp.(f p)*(g p)</code>, or in Python as <code>lambda f, g: lambda p:
f(p)*g(p)</code>; and we consider constants such as <code>2</code> to denote constant
functions like <code>K 2</code> or <code>lambda p: 2</code>.</p>
<p>But what is this mysterious <code>p</code> argument?  It’s a context or point in
this multidimensional possibility space mentioned eralier, the one
that’s usually left implicit, so it needs to include all the axes of
variation we were talking about earlier; to get traditional APL
semantics, you would want it to be something like a stack of numbers,
but dicts are more fashionable these days than stacks, arrays, or
lists, so let’s consider it to be something like a Lisp alist indexed
by symbols, each symbol denoting some axis of variation.</p>
<p>So now we can interpret this line:</p>
<pre><code>proj = normal * XYZ.sum(raydir * normal)
</code></pre>
<p>as meaning (in Python):</p>
<pre><code>def proj(p):
    return normal(p) * sum(raydir(q) * normal(q)
                           for q in XYZ.possibilities_augmenting(p))
</code></pre>
<p>Here <code>possibilities_augmenting</code> is a method of the <code>XYZ</code> dimension
that gives you versions of the point <code>p</code> with all possible values of
<code>XYZ</code> substituted into it.  Thus the first call to <code>normal</code> might
return either the <code>x</code>, the <code>y</code>, or the <code>z</code> component of some
particular normal; but all three of those will be multiplied by the
same dot product.</p>
<p>Of course, this is not intended to suggest that it must be calculated
in this fashion, which would be immensely wasteful; it’s intended as a
specification of the relationship between inputs and outputs.</p>
<p>This suggests an implementation of the <code>vec</code> function mentioned
earlier, which in the C program was a <code>struct</code> type:</p>
<pre><code>def vec(x, y, z):
    values = {XYZ.x: x, XYZ.y: y, XYZ.z: z}
    return lambda p: values[p[XYZ]](p.without(XYZ))
</code></pre>
<p>That is, the functions produced by <code>vec</code> consume the <code>XYZ</code> dimension
of their input and dispatch to any of the three functions that were
passed in as their X, Y, and Z components.  So this expression from
the <code>pixel_color</code> function:</p>
<pre><code>vec(xx/ww-.5, yy/hh-.5, 1)
</code></pre>
<p>when invoked with <code>z</code> will dispatch to the constant function denoted
by <code>1</code>; when invoked with <code>y</code>, will dispatch to the function denoted
by <code>yy/hh-.5</code>, which itself will dispatch to <code>yy</code>, which in this
program varies by pixel, and to <code>hh</code>, which doesn’t vary at all during
a run of the program, and to another constant function that returns
0.5.</p>
<p>Another useful higher-order function is a “renaming” or “aliasing” or
“axis rotation” or “reshaping” function which turns one axis into
another:</p>
<pre><code>def rename(a, b, f):
    return lambda p: f(p.without(a).with(b, p[a]))
</code></pre>
<p>Considered spatially, this prevents <code>f</code> from varying over axis <code>a</code>,
rotating the motion of <code>p</code> along <code>a</code> into motion along the new axis
<code>b</code>.  Considered relationally, this renames column <code>b</code> of the inputs
to relation <code>f</code> to <code>a</code>.  This is the operator you need for carrying
out explicit outer products; if <code>f</code> and <code>g</code> are both vectors on axis
<code>b</code>, then <code>rename(a,b,f)+g</code> gives you their outer product sums, with
the values of <code>f</code> varying along the new axis <code>a</code> and the values of <code>g</code>
varying along axis <code>b</code> as before.  (This <code>rename</code> function also gives
you general axis transposition, in a sense.)</p>
<p>This “context” or “point” object may carry a whole collection of
context attributes with it that most of these functions don’t bother
to access, and can pass along to their callees without mentioning them
explicitly.</p>
<p>(If we think in terms of N-ary relations rather than in terms of
functions, you could think of this “point” as a query-by-example
partial record.  But that’s not very consistent with the functional
semantics described above.)</p>
<p>In theory, if all of your component functions being combined by means
such as lifted operators, <code>rename</code>, axis-construction functions like
<code>vec</code>, and dummy-axis-introduction functions like <code>XYZ.sum</code>, have
finite discrete domains along some axis, you ought to be able to
compare those domains and detect mismatches, and then you ought to be
able to describe the multidimensional domain of the combined function.
This is a lot like type-checking.  You might even be able to do it at
compile time, and if you have compile-time axis variables analogous to
type variables in parametric polymorphism, you might be able to do the
type-checking polymorphically at compile time.</p>
<p>(Also note that this eliminates run-time bounds-checking, just as
structs do.)</p>
<p>APL has some axis-transformation functions: compress, expand, take,
drop, and the sort of hidden operation of indexing a vector by another
vector, which is like binary relation composition or like a different
form of compress.  You could consider these either as generating new
axes or as subsetting the domain along an existing axis.  In APL, it’s
the former, and so you have to be careful to compress all the
attributes you care about by the same boolean vector, or index all of
them through the same index vector.</p>
<p>It seems like it might be more useful here to implicitly intersect
domains along the same axis, which is after all what we are doing when
we implicitly take the outer product of a scalar and a vector.
However, the operation of <em>obtaining the valid indices</em> along some
axis or all axes (i.e. ρ) then must introduce a new axis to arrange
its results along.</p>
<h2>Inter-loop dependencies</h2>
<p>So far, all of the above, however nicely it motivates and elaborates
APL’s default rules for conformability and broadcasting, only covers
scalar operations and nearly trivially parallelizable vector
operations with no interloop dependencies.  Operations like reverse,
rotate, grade-up (indirect sort), scan (prefix sum), and even take and
drop don’t treat the points along the axis as floating in space
independently, but rather having a total order, with even predecessor
and successor operations, and correspond in languages like C to loops
with interloop dependencies.</p>
<p>I can imagine a bunch of different possible ways to handle these: all
axes could be ordinal; grade-up could create an ordinal axis from a
non-ordinal axis or axis subset, and scan, reverse, rotate, take, and
drop could simply fail to compile when applied to non-ordinal axes;
instead of rotation you might have a “next index” or “previous index”
function which, since it knows which axis it’s acting along, knows
when to wrap; and so on.</p>
<p>This is an important area to do a good job in, and there will be
nonobvious interactions among factors.  These are, of course, the
areas in which ALGOL-family programs have to declare data structures
and SQL optimizers start having to plan out join plans, so we
shouldn’t expect easy wins in this area.</p>
<p>My raytracer example is in some sense carefully chosen to minimize
this; it constructs almost no intermediate data structures, unless you
count 3-vectors as “data structures”.</p>
<h2>Efficiency</h2>
<p>GPUs, but also multithreading and SIMD instructions and cache prefetch
and improved locality by avoiding memory access to unused columns.
“Blocking” or “tiling” for efficiency; also “deforestation”.</p>
<p>Parallel prefix sum and parallel sorting are well-studied problems.
To the extent that these operations are sufficient to efficiently
solve computational problems, we should expect programs written in
this fashion to benefit from fine-grained parallelism more easily than
regular programs.</p>
<hr />
<p>Rethinking this again, the basic idea is that some variables have
values that depend on the circumstances, and there are a variety of
circumstances (or dimensions or scales) that may or may not be
relevant to the value of any given variable.  The latitude and
longitude of each house on a block are different, but perhaps we
consider the temperature of the entire block to be identical — but it
varies by time of day, which the latitude and longitude do not.</p>
<p>There are different kinds of dimensions; Stanley Smith Stevens
described them as “levels of measurement”.  Some are
categorical/nominal rather than quantitative; quantitative dimensions
can be ordinal (sortable), interval dimensions (subtractable;
affine?), or ratio dimensions (with a zero element and thus
divisible).  Also, quantitative dimensions may be discrete or
continuous.</p>
<p>Pointwise operations on variables are straightforward.</p>
<hr />
<p>Rethinking yet again, the idea is sort of that each variable is sort
of a function of other variables:</p>
<pre><code>f = a * b + c
</code></pre>
<p>Or we could say it invokes those other variables as subroutines, and
eventually it bottoms out in inputs (the dimensions).  So the above
statement is isomorphic to something similar to this:</p>
<pre><code>def f(x, y, t):
    return a(x, y) * b(y) + c(x, t)
</code></pre>
<p>But when we quantify over a dimension (or, perhaps, even a dependent
variable?) we are generating an argument locally rather than merely
passing it along; similarly when we index, which is a form of
composition!</p>
<pre><code>f = c + +/[x] a * b
</code></pre>
<p>If <code>+/[x]</code> is summing along x, this decodes to something like the
following:</p>
<pre><code>def f(x, y, t):
    return sum(a(xi, y) * b(y) for xi in xs) + c(x, t)
</code></pre>
<p>The difference, of course, is that all this default parameterization
is purely implicit.</p>
<p>This is closely analogous to dynamic scoping in Lisp.</p>
<h2>Comparison to GNU MathProg and ZIMPL</h2>
<p>GNU MathProg, also known as GMPL, and ZIMPL are two languages for
defining models (“linear programs”) for a linear optimizer to
optimize.  (This is called “linear programming”.  See <a href="../notes/linear-optimization-landscape.html">Some notes on the landscape of linear optimization software and applications</a> for details.)  They have essentially
the same data model; in what follows I will use the MathProg
terminology and syntax.</p>
<p>I wish I had read about these systems earlier, because much of what I
describe above is already present in them.  Unfortunately, I didn’t
learn anything about them until 2019.</p>
<p>Here is a slightly tweaked example of a MathProg model from the GLPK
distribution, licensed under the GNU GPL version 2.  It describes some
kind of industrial metallurgy planning problem, perhaps finding the
cheapest way to mix two tonnes of an alloy within desired
concentration limits from a combination of recycled scrap and fresh
metal.</p>
<pre><code>/* plan.mod */
var bin1, &gt;= 0, &lt;= 200;    var bin2, &gt;= 0, &lt;= 2500;
var bin3, &gt;= 400, &lt;= 800;  var alum, &gt;= 0;
var silicon, &gt;= 0;
param sival := .38;

minimize
value: .03 * bin1 + .08 * bin2 + .17 * bin3 + .21 * alum + sival * silicon;

subject to
yield: bin1 + bin2 + bin3 + alum + silicon = 2000;
fe: .15 * bin1 + .04 * bin2 + .02 * bin3 + .01 * alum + .03 * silicon &lt;= 60;
cu: .03 * bin1 + .05 * bin2 + .08 * bin3 + .01 * alum &lt;= 100;
mn: .02 * bin1 + .04 * bin2 + .01 * bin3 &lt;= 40;
mg: .02 * bin1 + .03 * bin2 &lt;= 30;
al: .70 * bin1 + .75 * bin2 + .80 * bin3 + .97 * alum &gt;= 1500;
si: 250 &lt;= .02 * bin1 + .06 * bin2 + .08 * bin3 +
    .01 * alum + .97 * silicon &lt;= 300;
</code></pre>
<p>(<code>glpsol -m plan.mod -o plan.out</code> reports that the optimal values for
the design variables are <code>bin1</code> ≈ 44.3, <code>bin2</code> ≈ 844, <code>bin3</code> ≈ 534,
<code>alum</code> ≈ 421, and <code>silicon</code> ≈ 156, resulting in a <code>value</code> of $307.46.)</p>
<p>In the above example, all the variables and parameters are scalars,
but MathProg also supports models incorporating aggregates.  Here’s
another tweaked example from the GLPK distribution:</p>
<pre><code>/* Chvatal V. (1980), Hard knapsack problems, Op. Res. 28, 1402-1411. */

param n &gt; 0 integer;
param log2_n := log(n) / log(2);
param k := floor(log2_n);
param a{j in 1..n} := 2 ** (k + n + 1) + 2 ** (k + n + 1 - j) + 1;
param b := 0.5 * floor(sum{j in 1..n} a[j]);
var x{1..n} binary;
maximize obj: sum{j in 1..n} a[j] * x[j];
s.t. cap: sum{j in 1..n} a[j] * x[j] &lt;= b;
data;
param n := 15;
</code></pre>
<p>In these languages, the expressed values — the values that expressions
can evaluate to — are called “elemental values”, and they can be
strings (which are mostly treated as opaque atoms and are sometimes
called “symbolic values”), integers, real numbers, logical or binary
values, or sets of these, called “elemental sets”.  Their denoted
values — the values that can be identified by names — include the
expressed values, and also “model objects”, which consist of “sets”,
“parameters”, “variables”, “constraints”, and “objectives”.  All of
the model objects other than objectives are “indexed” by n-tuples
drawn from some “subscript domain” which is the Cartesian product of a
possibly-empty sequence of sets.</p>
<p>The logical or binary values are <em>true</em> and <em>false</em>.  The other kinds
of elemental values have their usual programming nature and support
the usual operations, including string concatenation, substrings,
transcendental functions, exponentiation, and set arithmetic.</p>
<p>Parameters and variables are essentially identical in structure; they
are partial functions from their subscript domains to elemental
values.  The only difference is that when you run the optimizer, you
choose the parameters, while the optimizer chooses the variables.
There are rules limiting the structure of numerical expressions to
ensure that they are linear in the variables; I will not be concerned
with that here, but that is the reason for thus drawing the
distinction at the language level rather than, for example, specifying
it with the objective.</p>
<p>Confusingly, there are two related entities called “sets”: the
expressed value, called an “elemental set”, and the model object,
which is a denoted value.  The difference is that the denoted value is
indexed, like parameters and variables; it is a partial function from
its subscript domain to elemental sets.</p>
<p>Constraints are partial functions from their subscript domains to
Boolean values.  The optimizer looks for a “feasible solution” which
makes them everywhere true, and which maximizes or minimizes the
objective among all feasible solutions.</p>
<p>The only iterative operations are <code>sum</code>, <code>prod</code>, <code>min</code>, and <code>max</code>,
<code>forall</code> and <code>exists</code> for sets, the implicit <code>forall</code> on each
constraint, and a set-comprehension operator on (elemental) sets that
amounts to a Cartesian product filtered by a logical expression.  All
of these implicitly include an elementwise transformation function.</p>
<p>There is, I think, no way to compute an elemental set whose contents
depend on the value of a variable, and the other kind of sets (the
model objects) are specified as part of the model, so they can’t
depend on the value of a variable either.  This serves the purpose of
ensuring that MathProg and ZIMPL models can be “translated” into the
formats MPS and CPLEX LP, which do not support any aggregate values at
all, only affine equations and inequalities in real, integer, and
logical variables.</p>
<p>Since (elemental) sets are the only aggregate values that expressions
can evaluate to, expressions as such can only describe pointwise
computations.  Consider this line from the above example:</p>
<pre><code>maximize obj: sum{j in 1..n} a[j] * x[j];
</code></pre>
<p>This defines an objective called <code>obj</code> which is to be maximized,
defined as Σ<em>ⱼ aⱼxⱼ</em>.  <code>1..n</code> is a range expression which produces a
set of 15 integers from the integer parameter <code>n</code> = 15; <code>{j in 1..n}</code>
declares the “dummy variable” <code>j</code> (confusingly, not related to the
meaning of “dummy variable” in statistics) which is scoped to the
“integrand” of <code>sum</code>, <code>a[j] * x[j]</code>, which indexes the real-valued
parameter <code>a</code> and the binary-valued parameter <code>x</code> with the 1-tuple
<code>j</code>, producing respectively a real and a binary value for a given
value of <code>j</code>, which are then multiplied together; and finally <code>sum</code>
produces a real number by summing 15 values computed from its
“integrand”.</p>
<p>This approach to aggregate operations is very different from the APL
approach, in which you would write <code>+/a×x</code>, which is at least
considerably fewer user interface operations.  In the principled APL
I’m thinking about, <code>+/</code> would need to somehow indicate which axis (or
dimension or index) it wants to sum along: <code>+/j a×x</code>, for example, if
the axis were in fact called <code>j</code>, or <code>(sum (j) (* a x))</code> in Lisp
S-expression syntax.  However, for reasons described in <a href="../notes/implicit-language.html">A formal language for defining implicitly parameterized functions</a>, I think this approach will probably lead to
variable-capture problems of the same kind encountered in Lisp macros,
and so it’s probably best to scope the variables bound by such
iterative operations lexically; this leads to the formulation <code>+/j
a[j=j]×x[j=j]</code>, which abbreviated to <code>+/j a[j]×x[j]</code> looks very
similar to the MathProg approach.  The biggest difference is that the
limits of the sum come from the domain of the “integrand” rather than
being explicitly specified.</p>
<p>(The indexing or reshaping operation <code>v[k=e]</code> is a special case: it
binds variables dynamically rather than lexically, since otherwise it
would not work.)</p>
<p>MathProg variables and parameters correspond roughly to the array
variables described above, and indexes and sets are conflated above as
“axes of variation” or “inputs”.  A big difference from APL is that in
MathProg elementary sets are first-class values, while in APL axes are
not any kind of value at all, except in the implicit sense that they
are functions from your program’s inputs to some dimension of some
array.  Also, sets in MathProg are unordered, while many operations in
APL only make sense if some axes are ordered sequences of index
values, notably prefix sum or scan <code>\</code>, encode <code>⊤</code> and decode <code>⊥</code> (the
names come from their use for number base conversion), grade-up <code>⍋</code>
and grade-down <code>⍒</code>, reversal and rotation <code>⌽</code>, take <code>↑</code>, drop <code>↓</code>, and
catenate <code>,</code>.</p><script src="../liabilities/addtoc.js"></script><div><h2>Topics</h2><ul><li><a href="../topics/performance.html">Performance</a> (149 notes)
</li><li><a href="../topics/graphics.html">Graphics</a> (91 notes)
</li><li><a href="../topics/programming-languages.html">Programming languages</a> (47 notes)
</li><li><a href="../topics/c.html">C</a> (28 notes)
</li><li><a href="../topics/python.html">Python</a> (27 notes)
</li><li><a href="../topics/arrays.html">Arrays</a> (17 notes)
</li><li><a href="../topics/simd.html">SIMD instructions</a> (10 notes)
</li><li><a href="../topics/apl.html">APL</a> (9 notes)
</li><li><a href="../topics/predicate-logic.html">Predicate logic</a> (6 notes)
</li><li><a href="../topics/types.html">Types</a> (5 notes)
</li><li><a href="../topics/gpgpu.html">GPGPU</a> (2 notes)
</li><li>Prefix sum</li></ul></div></html>